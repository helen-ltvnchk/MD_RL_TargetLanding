# Отчёт по лабораторной работе: Реализация PPO-агента для задачи посадки БПЛА

## Цели и задачи

* Реализовать базовый алгоритм обучения с подкреплением.
* Провести первые эксперименты с агентом в выбранной среде.

## Выбранный алгоритм: PPO

Алгоритм **Proximal Policy Optimization (PPO)** был выбран по следующим причинам:

* **Стабильность обучения:**
  PPO ограничивает обновления политики, предотвращая резкие скачки в поведении агента.

* **Работа с непрерывным пространством действий:**
  В отличие от Q-Learning и DQN, PPO отлично работает с непрерывными действиями без дополнительных ухищрений.

* **Совместимость с Stable Baselines3:**
  Полная поддержка в SB3 позволила быстро интегрировать алгоритм и сосредоточиться на обучении и экспериментах.

* **Широкое применение:**
  PPO широко используется в задачах управления и считается одним из наиболее надёжных алгоритмов для RL.

## Среда: DroneLandingEnv

* Симулируется задача посадки дрона в 2D-пространстве.
* Агент управляет вертикальной и горизонтальной тягой.
* Среда учитывает:

  * Гравитацию
  * Расход топлива
  * Стохастические начальные условия

### Изменения среды для обучения агента:

* Переход от `gym` к `gymnasium` для совместимости со Stable Baselines3.
* Обновлены методы `reset()` и `step()` под требования `gymnasium`.
* Переписана функция награды (**reward shaping**), чтобы ускорить и направить обучение агента.

## ⚖️ Reward Shaping

Для ускорения обучения и формирования желаемого поведения была реализована следующая система наград:

* **Бонус за приближение к цели**
* **Бонус за снижение высоты**
* **Штраф за высокую скорость**
* **Бонус за мягкую посадку**


## Этапы обучения

1. **Первый этап** — обучение на 300 000 шагов с базовой функцией награды.
2. **Второй этап** — обучение на дополнительных 100 000 шагов с улучшенным `reward shaping`.

Во втором этапе агент начал стабильно достигать цели и совершать мягкие посадки.

##  Результаты

* График изменения эпизодической награды показывает рост и стабилизацию около **+105**.
* Агент научился тратить топливо рационально и уменьшать скорость при посадке.
* Возрос процент успешных посадок.

## Заключение

Алгоритм PPO оказался эффективным для задачи посадки БПЛА. Использование reward shaping и адаптация среды под gymnasium позволили добиться устойчивого обучения агента. Результаты подтверждают применимость методов обучения с подкреплением в задачах управления физическими системами. Несмотря на уже довольно высокую награду, дрон всё ещё не приближается достаточно близко к точке посадки. В дальнейшем планируется дальнейшая работа с `reward shaping`, для более точной посадки
