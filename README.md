# Target Landing: Посадка БПЛА в заданную точку

## Описание задачи

Цель задачи — обучить беспилотный летательный аппарат (БПЛА) выполнять **точную посадку** в заданную целевую точку. Агент управляет тягой двигателей, чтобы:

- **Минимизировать отклонение от цели**.
- **Сохранять устойчивость** (малая скорость при посадке).
- **Эффективно расходовать ресурсы** (топливо).

Задача моделируется в **2D-пространстве** с учетом физики (гравитация, тяга двигателей). Начальные условия (например, положение и высота дрона) генерируются случайным образом, что делает задачу стохастической.

---

## Цели

1. **Точность**: Обеспечить посадку дрона как можно ближе к целевой точке.  
2. **Безопасность**: Убедиться, что скорость дрона при посадке минимальна, чтобы избежать повреждений.  
3. **Эффективность**: Минимизировать расход топлива и время выполнения задачи.

---

## Ограничения

1. **Физические ограничения**:
   - Гравитация постоянно действует на дрон, требуя управления вертикальной тягой.  
   - Максимальная тяга двигателей ограничена.  

2. **Ресурсные ограничения**:
   - Дрон имеет ограниченный запас топлива, который расходуется при использовании двигателей.  

3. **Стохастичность среды**:
   - Начальные условия (положение и высота) случайны, что усложняет обучение универсальной стратегии.  

4. **Условия завершения**:
   - Падение (высота $ < 0 \, \text{м} $).  
   - Исчерпание топлива.  
   - Превышение максимального количества шагов ($ 500 $).

---

## Почему обучение с подкреплением (RL)?

1. **Гибкость RL**:
   - RL позволяет обучать агента в динамической среде, где аналитическое решение затруднено из-за сложности физики и стохастичности начальных условий.  
   - Агент может адаптироваться к различным начальным позициям и внешним условиям (например, ветер, если добавить его в будущем).  

2. **Обучение через взаимодействие**:
   - В отличие от классических подходов (например, предварительно запрограммированных траекторий), RL не требует явного задания правил управления.  
   - Агент исследует пространство действий и учится на собственных ошибках, находя оптимальные стратегии посадки.  

3. **Оптимизация награды**:
   - RL позволяет одновременно оптимизировать несколько целей через функцию награды:
     - Приближение к цели.  
     - Поддержание низкой скорости.  
     - Экономию топлива.  
   - Например, можно поощрять успешные посадки и штрафовать за падения или избыточное потребление топлива.  

4. **Масштабируемость**:
   - После обучения RL-агент может быть перенесён на более сложные задачи (например, 3D-посадку или посадку в условиях ветра).  
   - Можно использовать симуляторы (например, OpenAI Gym, PyBullet) для безопасного и быстрого обучения перед реальным применением.

---

## Пример реализации

1. **Среда**:
   Реализовать симуляцию движения дрона в 2D-пространстве с учетом физики (гравитация, тяга двигателей).  

2. **Пространство состояний**:
   - Положение дрона $(x, y)$.  
   - Скорость $(v_x, v_y)$.  
   - Расстояние до цели.  
   - Остаток топлива.  

3. **Пространство действий**:
   - Управление тягой двигателей:
     - Горизонтальная составляющая (движение влево/вправо).  
     - Вертикальная составляющая (подъем/спуск).  

4. **Функция награды**:
   - Положительная награда за успешную посадку.  
   - Штрафы за:
     - Отклонение от цели.  
     - Высокую скорость при посадке.  
     - Расход топлива.  
     - Падение или исчерпание топлива.  

---

## Вывод

Target Landing — это сложная задача с множеством факторов, влияющих на успех (физика, ресурсы, стохастичность). Обучение с подкреплением идеально подходит для её решения благодаря способности к обучению через взаимодействие, адаптивности и возможности оптимизации нескольких целей одновременно. Использование RL позволяет создать универсального агента, способного справляться с разнообразными условиями посадки.

---


